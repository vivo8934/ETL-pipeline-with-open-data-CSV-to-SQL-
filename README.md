
# ğŸ“Š Weather Events ETL Pipeline Project

This project implements a scalable ETL pipeline to ingest, clean, and store 8.6 million+ weather event records into a PostgreSQL database using Docker. It also includes a Streamlit dashboard for data exploration and pgAdmin for database inspection.



## âœ… Project Goals

- ğŸš€ Efficiently handle large CSV files with chunked loading.
- ğŸ§¹ Clean and normalize weather event data.
- ğŸ—ƒï¸ Store structured data in PostgreSQL.
- ğŸ§ª Ensure quality with testing and validation.
- ğŸ“Š Expose a dashboard for real-time exploration (Streamlit).
- ğŸ› ï¸ Provide easy database access (pgAdmin).

## ğŸ§± Tech Stack

- Python, Pandas
- PostgreSQL (via Docker)
- SQLAlchemy
- Streamlit
- pgAdmin
- Docker + Docker Compose

# ğŸš€ How to start the project locally 

- ğŸ‘‰ git clone - https://github.com/vivo8934/ETL-pipeline-with-open-data-CSV-to-SQL-.git
- ğŸ‘‰ navigate to the folder **ETL-pipeline-with-open-data-CSV-to-SQL**
- ğŸ‘‰ pip install -r requirements.txt

## ğŸ—ƒï¸ Dataset Setup (External Download)

This project processes a large CSV file (~1GB) containing historical weather events. Due to GitHub's 100MB file size limit, we **do not include the CSV file in the Git repository**.

### ğŸ“¥ Download the CSV File

Download the dataset programmatically using the following options below:

1. Download from this link:  
   ğŸ‘‰ [Google Drive - WeatherEvents_Jan2016-Dec2022.csv](https://drive.google.com/file/d/1LucwOt_ez1-yT5dUMxr-uwPdt-4iLa3F/view?usp=drive_link)

2. Download Package to get the file on the project Folder on your PC 
    ğŸ‘‰ pip install gdown
    ğŸ‘‰ gdown --id 1LucwOt_ez1-yT5dUMxr-uwPdt-4iLa3F -O /YOUR_PROJECT_PATH/WeatherEvents_Jan2016-Dec2022.csv

3. Don't Track the CSV File in Git 
    ğŸ‘‰ echo "WeatherEvents_Jan2016-Dec2022.csv" >> .gitignore
    ğŸ‘‰ git rm --cached WeatherEvents_Jan2016-Dec2022.csv
    ğŸ‘‰ git commit -m "Remove large CSV file from repo"
    ğŸ‘‰ git push origin main

## ğŸ› ï¸ PostgreSQL Setup with Docker

This project uses **PostgreSQL** as the target database for loading transformed weather data. A Dockerized PostgreSQL instance is used for ease of development and isolation.

### ğŸ“¦ Step 1: Environment Configuration

Create a `.env` file in the project root with the following:

```env
DB_USER=etl_user
DB_PASSWORD=etl_pass
DB_HOST=localhost
DB_PORT=5432
DB_NAME=weather_db

# Optional: CSV file path (default is shown below)
CSV_PATH=data/WeatherEvents_Jan2016-Dec2022.csv

docker compose -f docker/docker-compose.yml up -d

- Make sure the file WeatherEvents_Jan2016-Dec2022.csv is not tracked in Git, but is placed manually into the data/ directory (see earlier section on dataset download).



